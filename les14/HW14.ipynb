{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53be16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# import torch\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd6906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:89: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, pipeline, AutoTokenizer, TFAutoModelForSequenceClassification, TFAutoModelWithLMHead, TFAutoModel, TFAutoModelForQuestionAnswering, TFAutoModelForTokenClassification, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488fd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpu = gpus[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1f870",
   "metadata": {},
   "source": [
    "#### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60904d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a584e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233a6a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_ids = tokenizer(sequence)\n",
    "inp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e350c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                   \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6a3312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_sequence = tokenizer.decode(inp_ids['input_ids'])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f25b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n",
    "encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596457f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 1188, 1110, 170, 1603, 4954, 119, 102],\n",
       " [101,\n",
       "  1188,\n",
       "  1110,\n",
       "  170,\n",
       "  1897,\n",
       "  1263,\n",
       "  4954,\n",
       "  119,\n",
       "  1135,\n",
       "  1110,\n",
       "  1120,\n",
       "  1655,\n",
       "  2039,\n",
       "  1190,\n",
       "  1103,\n",
       "  4954,\n",
       "  138,\n",
       "  119,\n",
       "  102])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequence_a, encoded_sequence_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412f9e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_sequence_a), len(encoded_sequence_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3c1580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c39a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sequences['input_ids'][0]),len(padded_sequences['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3efdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1188, 1110, 170, 1603, 4954, 119, 102, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69d7f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This is a short sequence. [SEP] This is a rather long sequence. It is at least longer than the sequence A. [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf67f7",
   "metadata": {},
   "source": [
    "### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3de79d",
   "metadata": {},
   "source": [
    "#### sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9eeeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee1c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Wonderful film, one of the best horror films of the 70s. She is realistic settings and atmospheres. As usual it was inevitable the usual negative comments. I have noticed that most horror films of a certain period many times fail to reach even sufficiency. Obviously because most horror movies are old and must be denigrati, is like a mental mechanism that moves the minds of the potential of music critics here.<br /><br />Before you read the review already knew what was the final judgment. In the film a good gift because 10 is really well done. Raines reads quite well and the film as a way in which it was produced reminds me a lot of Kubrick films. He really impression. Excellent film really. I consider a film anthology of years'70.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cadf2f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996466636657715}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db2e05",
   "metadata": {},
   "source": [
    "#### paraphrase check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b97f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sentence_1 = \"Apples are especially bad for your health\"\n",
    "sentence_2 = \"HuggingFace's headquarters are situated in Manhattan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e9b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb15f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_1 = tokenizer(sentence_0, sentence_2, return_tensors=\"tf\")\n",
    "seq_2 = tokenizer(sentence_0, sentence_1, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff25034",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_clf_logits_1 = model(seq_1)[0]\n",
    "seq_clf_logits_2 = model(seq_2)[0]\n",
    "\n",
    "seq_res_1 = tf.nn.softmax(seq_clf_logits_1)\n",
    "seq_res_2 = tf.nn.softmax(seq_clf_logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c38e90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(np.array(seq_res_1)[0][i] * 100))}%\")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(np.array(seq_res_2)[0][i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fddd6",
   "metadata": {},
   "source": [
    "### question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21163c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "821cad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c56fa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6177279949188232,\n",
       " 'start': 34,\n",
       " 'end': 95,\n",
       " 'answer': 'the task of extracting an answer from a text given a question'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(question=\"What is extractive question answering?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eca0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2012b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7063797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "924d2eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    \n",
    "    answer_start = tf.argmax(\n",
    "        answer_start_scores, axis=1\n",
    "    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = (\n",
    "        tf.argmax(answer_end_scores, axis=1) + 1\n",
    "    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "961a0b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,   100, 19081,  3640,  6970, 25918,  8010,  2090,  2029,\n",
       "        7705,  2015,  1029,   102,   100, 19081,  1006,  3839,  2124,\n",
       "        2004,  1052, 22123,  2953,  2818,  1011, 19081,  1998,  1052,\n",
       "       22123,  2953,  2818,  1011,  3653, 23654,  2098,  1011, 14324,\n",
       "        1007,  3640,  2236,  1011,  3800,  4294,  2015,  1006, 14324,\n",
       "        1010, 14246,  2102,  1011,  1016,  1010, 23455,  1010, 28712,\n",
       "        2213,  1010,  4487, 16643, 23373,  1010, 28712,  7159,  1529,\n",
       "        1007,  2005,  3019,  2653,  4824,  1006, 17953,  2226,  1007,\n",
       "        1998,  3019,  2653,  4245,  1006, 17953,  2290,  1007,  2007,\n",
       "        2058,  3590,  1009,  3653, 23654,  2098,  4275,  1999,  2531,\n",
       "        1009,  4155,  1998,  2784,  6970, 25918,  8010,  2090, 23435,\n",
       "       12314,  1016,  1012,  1014,  1998,  1052, 22123,  2953,  2818,\n",
       "        1012,   102])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8d05ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFQuestionAnsweringModelOutput(loss=None, start_logits=<tf.Tensor: shape=(1, 110), dtype=float32, numpy=\n",
       "array([[-6.14786   , -8.150385  , -8.054717  , -8.2172    , -8.259663  ,\n",
       "        -9.034024  , -9.183441  , -8.518948  , -7.0240912 , -8.260817  ,\n",
       "        -8.871909  , -9.769736  , -6.147826  , -5.6884265 , -3.57987   ,\n",
       "        -7.2198443 , -5.8089457 , -7.530818  , -7.7309594 , -0.23078305,\n",
       "        -6.265777  , -6.5939264 , -5.9500937 , -7.4640136 , -4.691924  ,\n",
       "        -6.0423026 , -0.76219726, -6.1282    , -6.799284  , -6.0876284 ,\n",
       "        -6.4823346 , -3.8884654 , -7.2929006 , -7.8000574 , -6.8442364 ,\n",
       "        -2.122364  , -6.978371  , -3.6053085 , -0.725004  , -7.1121693 ,\n",
       "        -4.889711  , -3.4081593 , -5.9891477 , -1.8025491 ,  2.1470108 ,\n",
       "        -5.6282454 , -1.4006665 , -6.57091   , -7.238273  , -5.816609  ,\n",
       "        -7.8011994 , -4.1402016 , -7.5582337 , -4.124343  , -6.8239703 ,\n",
       "        -8.069542  , -4.9949927 , -7.3041825 , -6.9205236 , -8.032051  ,\n",
       "        -5.2738614 , -6.0295787 , -6.344321  , -6.4113092 , -3.201203  ,\n",
       "         1.8201444 , -4.3072486 , -5.1025605 , -2.3548028 , -0.6008438 ,\n",
       "        -6.1211553 , -6.490723  , -6.027909  , -2.2168102 , -6.113441  ,\n",
       "        -5.678305  , -5.463614  , -5.1583123 , -6.109006  , -5.331981  ,\n",
       "        -6.8291273 , -6.343497  , -7.265545  , -8.59959   , -6.2426906 ,\n",
       "        -8.210697  , -8.521333  , -5.505159  , -7.9097924 , -6.970858  ,\n",
       "        -8.895678  , -5.391948  , -6.2109904 , -1.8061259 , -3.2943351 ,\n",
       "        -5.5693564 , -5.5466037 , -0.27026802,  6.1828256 , -1.8694721 ,\n",
       "        -1.4109625 , -4.2709975 , -3.979462  , -3.7275198 , -1.1112503 ,\n",
       "        -4.8465014 , -5.675876  , -3.7093496 , -4.5269475 , -6.148106  ]],\n",
       "      dtype=float32)>, end_logits=<tf.Tensor: shape=(1, 110), dtype=float32, numpy=\n",
       "array([[-1.7815244 , -6.9373865 , -7.7163377 , -8.572278  , -8.4385195 ,\n",
       "        -8.14141   , -7.887155  , -7.985988  , -6.973628  , -7.468175  ,\n",
       "        -7.5531373 , -7.9217653 , -1.7814938 , -5.1120224 , -3.1625652 ,\n",
       "        -6.5965843 , -7.4106226 , -7.1400733 , -7.221417  , -4.972221  ,\n",
       "        -5.650024  , -5.974436  , -1.9262793 , -6.4548674 , -2.333113  ,\n",
       "        -6.4600396 , -5.031788  , -5.5209594 , -5.676476  , -1.5702851 ,\n",
       "        -6.4762297 , -6.1248016 , -6.1384115 , -4.6933937 , -6.0532064 ,\n",
       "         0.68273133, -4.949974  , -6.9683557 , -5.423082  , -7.3902183 ,\n",
       "        -4.484889  , -3.2898028 , -1.3844324 , -6.217081  , -3.2169368 ,\n",
       "        -4.0713377 , -4.8511095 , -4.4140606 , -5.862881  , -0.68475455,\n",
       "        -2.3606718 , -1.6260966 , -2.8481622 , -5.2226696 , -2.4681287 ,\n",
       "        -3.0868385 , -4.764742  , -5.769306  , -1.8202595 , -3.3358476 ,\n",
       "        -4.931521  ,  0.47377086, -0.9115661 , -2.2152922 , -5.6263366 ,\n",
       "        -3.6995752 , -3.6446722 , -1.9956731 , -6.264887  , -4.664155  ,\n",
       "        -3.5040913 , -2.6142237 , -5.355563  , -4.389304  , -3.5511258 ,\n",
       "         0.20405127, -6.0317736 , -5.07793   , -0.3174603 ,  2.2365413 ,\n",
       "        -3.269397  , -6.349304  , -6.120542  , -6.4000673 , -6.581241  ,\n",
       "        -6.4828396 , -5.640953  , -2.9006014 , -6.949198  , -6.4214506 ,\n",
       "        -6.376462  , -2.4753783 , -5.636876  , -4.77979   , -6.639887  ,\n",
       "        -6.34219   , -4.2502575 , -4.7064877 , -2.5913155 , -0.73751307,\n",
       "        -3.0311935 , -4.155017  , -0.5356362 , -2.6971984 , -1.7214199 ,\n",
       "        -2.4598606 , -2.964052  ,  6.593194  ,  5.1990204 , -1.7820569 ]],\n",
       "      dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef8a6834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 108)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start, answer_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2426f5a",
   "metadata": {},
   "source": [
    "### –Ø–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac8945",
   "metadata": {},
   "source": [
    "#### fill mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d67970eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17927466332912445,\n",
      "  'sequence': 'HuggingFace is creating a tool that the community uses to solve '\n",
      "              'NLP tasks.',\n",
      "  'token': 3944,\n",
      "  'token_str': ' tool'},\n",
      " {'score': 0.11349395662546158,\n",
      "  'sequence': 'HuggingFace is creating a framework that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 7208,\n",
      "  'token_str': ' framework'},\n",
      " {'score': 0.05243542045354843,\n",
      "  'sequence': 'HuggingFace is creating a library that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 5560,\n",
      "  'token_str': ' library'},\n",
      " {'score': 0.03493538498878479,\n",
      "  'sequence': 'HuggingFace is creating a database that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 8503,\n",
      "  'token_str': ' database'},\n",
      " {'score': 0.028602542355656624,\n",
      "  'sequence': 'HuggingFace is creating a prototype that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 17715,\n",
      "  'token_str': ' prototype'}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\")\n",
    "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f67e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d23a5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_tf_auto.py:593: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = TFAutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19f58834",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ada292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "\n",
    "mask_token_index = tf.where(inp == tokenizer.mask_token_id)[0, 1]\n",
    "token_logits = model(inp)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d442a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35979b",
   "metadata": {},
   "source": [
    "#### text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3db34781",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c36ba318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]\n"
     ]
    }
   ],
   "source": [
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fc246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4687aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLNetLMHeadModel.\n",
      "\n",
      "All the layers of TFXLNetLMHeadModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e062395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9360533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on anning on taking a walk around the grounds of the Cathedral of Saint Paul, which is on the edge of the lake on the banks of the Volga. During the morning, the sun gets high enough to give a dramatic picture of the lake, and the sunset to be spectacular. The lake is situated about 25 miles from the city center, along a river which flows into the Vol\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"tf\")\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85f7b0",
   "metadata": {},
   "source": [
    "#### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ce9efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9603e23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-ORG',\n",
       "  'score': 0.9995786,\n",
       "  'index': 1,\n",
       "  'word': 'Hu',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9909764,\n",
       "  'index': 2,\n",
       "  'word': '##gging',\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9982225,\n",
       "  'index': 3,\n",
       "  'word': 'Face',\n",
       "  'start': 8,\n",
       "  'end': 12},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99948806,\n",
       "  'index': 4,\n",
       "  'word': 'Inc',\n",
       "  'start': 13,\n",
       "  'end': 16},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99943453,\n",
       "  'index': 11,\n",
       "  'word': 'New',\n",
       "  'start': 40,\n",
       "  'end': 43},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9993196,\n",
       "  'index': 12,\n",
       "  'word': 'York',\n",
       "  'start': 44,\n",
       "  'end': 48},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9993794,\n",
       "  'index': 13,\n",
       "  'word': 'City',\n",
       "  'start': 49,\n",
       "  'end': 53},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.98625827,\n",
       "  'index': 19,\n",
       "  'word': 'D',\n",
       "  'start': 79,\n",
       "  'end': 80},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.951427,\n",
       "  'index': 20,\n",
       "  'word': '##UM',\n",
       "  'start': 80,\n",
       "  'end': 82},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.93365914,\n",
       "  'index': 21,\n",
       "  'word': '##BO',\n",
       "  'start': 82,\n",
       "  'end': 84},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.97616535,\n",
       "  'index': 28,\n",
       "  'word': 'Manhattan',\n",
       "  'start': 114,\n",
       "  'end': 123},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9914629,\n",
       "  'index': 29,\n",
       "  'word': 'Bridge',\n",
       "  'start': 124,\n",
       "  'end': 130}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"\"\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO,\n",
    "therefore very close to the Manhattan Bridge which is visible from the window.\"\"\"\n",
    "nlp(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63f4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac668a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TFAutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") #OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9fe5a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
    "#            \"close to the Manhattan Bridge.\"\n",
    "# # Bit of a hack to get the tokens with the special tokens\n",
    "# tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "# inputs = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "# outputs = model(inputs)[0]\n",
    "# predictions = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf0d9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "#     print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a179e",
   "metadata": {},
   "source": [
    "#### summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15442f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1fcea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b331cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002 . At one time, she was married to eight men at once, prosecutors say .'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23759b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bd36d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\") #OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2031ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"tf\", max_length=512)\n",
    "outputs = model.generate(inputs, max_length=130, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3919dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> prosecutors say the marriages were part of an immigration scam. if convicted, barrientos faces two criminal counts of \"offering a false instrument\"\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60d069",
   "metadata": {},
   "source": [
    "#### translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7a920e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris.'}]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_de\")\n",
    "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a17f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1401ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\") #OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cbc7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"tf\")\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0391b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df2e96",
   "metadata": {},
   "source": [
    "### Fine-tuning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7fe9e",
   "metadata": {},
   "source": [
    "#### case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bfc1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acb6fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30942f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\snetkova\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7922af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f97adbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\snetkova\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-1a0d43b4b87f8539.arrow\n",
      "Loading cached processed dataset at C:\\Users\\snetkova\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-e60c541bef0cd917.arrow\n",
      "Loading cached processed dataset at C:\\Users\\snetkova\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-2835c761fdfb6608.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    examples['text'] = [x[:150] for x in examples['text']] # –≤–æ–∑—å–º–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤, –∏–Ω–∞—á–µ –Ω–µ –ª–µ–∑–µ—Ç –≤ –ø–∞–º—è—Ç—å GPU\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e0974a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\snetkova\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-166ea9626137b36a.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\snetkova\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-fe412d66927f47c5.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e360a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–Ω–∞—á–∞–ª–∞ –º—ã —É–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü ¬´—Ç–µ–∫—Å—Ç¬ª –∏–∑ –Ω–∞—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ TensorFlow:\n",
    "tf_train_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "tf_eval_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9571426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snetkova\\Anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –≤ –±–æ–ª—å—à–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º tf.data.Dataset.from_tensor_slices –º–µ—Ç–æ–¥: \n",
    "train_features = {x: tf_train_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(1)\n",
    "\n",
    "eval_features = {x: tf_eval_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
    "eval_tf_dataset = eval_tf_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc8f0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b51226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000023B690EB7B8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000023B690EB7B8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.7177 - sparse_categorical_accuracy: 0.4840WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "500/500 [==============================] - 95s 160ms/step - loss: 0.7177 - sparse_categorical_accuracy: 0.4840 - val_loss: 0.6917 - val_sparse_categorical_accuracy: 0.5600\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 80s 161ms/step - loss: 0.7197 - sparse_categorical_accuracy: 0.5020 - val_loss: 0.6953 - val_sparse_categorical_accuracy: 0.5300\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 82s 164ms/step - loss: 0.7019 - sparse_categorical_accuracy: 0.4860 - val_loss: 0.7035 - val_sparse_categorical_accuracy: 0.4700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b9e8f4908>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1df1e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_imdb_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23c97dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\"my_imdb_model\", from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38b5ec12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6cc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd8b4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert = TFAutoModel.from_pretrained(\"Geotrend/bert-base-ru-cased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-ru-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3068f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok = tokenizer([\"–ü–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è –æ—Ç–ª–∏—á–Ω–∞—è –≤ –æ—Ç–ª–∏—á–∏–∏ –æ—Ç –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–∏.\", \"–í–µ—á–µ—Ä–æ–º –∏–¥—ë—Ç —Å–∏–ª—å–Ω—ã–π –¥–æ–∂–¥—å –æ–±—ã—á–Ω–æ –¥–æ —É—Ç—Ä–∞!\"], \n",
    "#                max_length=20, truncation=True, padding='max_length', return_token_type_ids=False, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "554d00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cf644bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = bert(**tok)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78c770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[0].numpy().shape, out[1].numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb709f",
   "metadata": {},
   "source": [
    "#### case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d94b35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
    "df = pd.read_csv('D:/GeekBrains/nlp/les14/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4218d5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a478f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bert = TFAutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13c34964",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bc1d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['Sentiment'].values  # take sentiment column in df as array\n",
    "labels = np.zeros((arr.size, arr.max()+1))  # initialize empty (all zero) label array\n",
    "labels[np.arange(arr.size), arr] = 1  # add ones in indices where we have a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "911dcfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to handle tokenization\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "# initialize two arrays for input tensors\n",
    "Xids = np.zeros((len(df), SEQ_LEN))\n",
    "Xmask = np.zeros((len(df), SEQ_LEN))\n",
    "\n",
    "# loop through data and tokenize everything\n",
    "for i, sentence in enumerate(df['Phrase']):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1d0a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# restructure dataset format for BERT\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "dataset = dataset.map(map_func)  # apply the mapping function\n",
    "\n",
    "# shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(5000).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "532ca329",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_LEN = len(list(dataset))  # get dataset length\n",
    "\n",
    "SPLIT = 0.9  # we will create a 90-10 split\n",
    "\n",
    "# create training-validation sets\n",
    "train = dataset.take(round(DS_LEN*SPLIT))\n",
    "val = dataset.skip(round(DS_LEN*SPLIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "704df2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, mask)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "X = tf.keras.layers.GRU(256)(embeddings)\n",
    "#X = tf.keras.layers.GlobalMaxPool1D()(X)  # reduce tensor dimensionality\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze the BERT layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3955e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 256)          787968      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          32896       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_524 (Dropout)           (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 5)            645         dropout_524[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 109,132,805\n",
      "Trainable params: 822,021\n",
      "Non-trainable params: 108,310,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a533d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model/gru/PartitionedCall]] [Op:__inference_train_function_900426]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-7d71971f6433>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model/gru/PartitionedCall]] [Op:__inference_train_function_900426]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f285f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9146b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b0bbb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 5849561199636670946,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2255906407\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6071586592004984124\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "788a0a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3da2311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-89-78f884b5c1a9>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86887934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
